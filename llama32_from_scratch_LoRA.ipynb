{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
      "metadata": {
        "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
      },
      "source": [
        "# Llama 3.2 From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nMF2sIKF7Kr7",
      "metadata": {
        "id": "nMF2sIKF7Kr7"
      },
      "outputs": [],
      "source": [
        "#Reqs:\n",
        "#blobfile>=3.0.0\n",
        "#huggingface_hub>=0.24.7\n",
        "#ipywidgets>=8.1.2\n",
        "#safetensors>=0.4.4\n",
        "#sentencepiece>=0.1.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "208c7e6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "208c7e6b",
        "outputId": "fa656fa6-04e3-42d3-ce70-8dfcef3d6623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2XBatVeHjiW8",
      "metadata": {
        "id": "2XBatVeHjiW8"
      },
      "outputs": [],
      "source": [
        "#%load_ext cuml.accel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
        "outputId": "dbb17909-a73f-4edd-d3b4-a0a45d1a2ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blobfile version: 3.0.0\n",
            "huggingface_hub version: 0.34.4\n",
            "tiktoken version: 0.11.0\n",
            "torch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"blobfile\",         # to download pretrained weights\n",
        "    \"huggingface_hub\",  # to download pretrained weights\n",
        "    \"tiktoken\",         # to implement the tokenizer\n",
        "    \"torch\",            # to implement the model\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "RscSaB-31GW6",
      "metadata": {
        "id": "RscSaB-31GW6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from safetensors.torch import load_file\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tiktoken\n",
        "from tiktoken.load import load_tiktoken_bpe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
      "metadata": {
        "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
      },
      "source": [
        "&nbsp;\n",
        "# 1. Architecture code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
      "metadata": {
        "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x = nn.functional.silu(x_fc1) * x_fc2\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
      "metadata": {
        "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
      },
      "outputs": [],
      "source": [
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None, dtype=torch.float32):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    # Compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    # Frequency adjustments\n",
        "    if freq_config is not None:\n",
        "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
        "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
        "\n",
        "        wavelen = 2 * torch.pi / inv_freq\n",
        "\n",
        "        inv_freq_llama = torch.where(\n",
        "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
        "        )\n",
        "\n",
        "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
        "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
        "        )\n",
        "\n",
        "        smoothed_inv_freq = (\n",
        "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
        "        )\n",
        "\n",
        "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
        "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
        "        inv_freq = inv_freq_llama\n",
        "\n",
        "    # Generate position indices\n",
        "    positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "    # Compute the angles\n",
        "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "    # Expand angles to match the head_dim\n",
        "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "    # Precompute sine and cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin\n",
        "\n",
        "\n",
        "def apply_rope(x, cos, sin):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1 = x[..., : head_dim // 2]  # First half\n",
        "    x2 = x[..., head_dim // 2 :]  # Second half\n",
        "\n",
        "    # Adjust sin and cos shapes\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply the rotary transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    # It's ok to use lower-precision after applying cos and sin rotation\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
      "metadata": {
        "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(\n",
        "            self, d_in, d_out, num_heads,\n",
        "            num_kv_groups,\n",
        "            dtype=None\n",
        "        ):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
        "\n",
        "    def forward(self, x, mask, cos, sin):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
        "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "        # Reshape queries, keys, and values\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
        "\n",
        "        # Transpose keys, values, and queries\n",
        "        keys = keys.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
        "        values = values.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
        "        queries = queries.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
        "\n",
        "        # Apply RoPE\n",
        "        keys = apply_rope(keys, cos, sin)\n",
        "        queries = apply_rope(queries, cos, sin)\n",
        "\n",
        "        # Expand keys and values to match the number of heads\n",
        "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
        "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
        "\n",
        "        # Compute scaled dot-product attention (self-attention) with a causal mask\n",
        "        # Shape: (b, num_heads, num_tokens, num_tokens)\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        assert keys.shape[-1] == self.head_dim\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
      "metadata": {
        "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = GroupedQueryAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "            dtype=cfg[\"dtype\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5, dtype=cfg[\"dtype\"])\n",
        "        self.norm2 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    def forward(self, x, mask, cos, sin):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x, mask, cos, sin)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
      "metadata": {
        "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
      },
      "outputs": [],
      "source": [
        "class Llama3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main model parameters\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
        "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        self.final_norm = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5, dtype=cfg[\"dtype\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "\n",
        "        # Reusuable utilities\n",
        "        cos, sin = compute_rope_params(\n",
        "            head_dim=cfg[\"emb_dim\"] // cfg[\"n_heads\"],\n",
        "            theta_base=cfg[\"rope_base\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            freq_config=cfg[\"rope_freq\"]\n",
        "        )\n",
        "        self.register_buffer(\"cos\", cos, persistent=False)\n",
        "        self.register_buffer(\"sin\", sin, persistent=False)\n",
        "        self.cfg = cfg\n",
        "\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        # Forward pass\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        x = tok_embeds\n",
        "\n",
        "        num_tokens = x.shape[1]\n",
        "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "\n",
        "        for block in self.trf_blocks:\n",
        "            x = block(x, mask, self.cos, self.sin)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
      "metadata": {
        "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
      },
      "source": [
        "&nbsp;\n",
        "# 2. Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "caa142fa-b375-4e78-b392-2072ced666f3",
      "metadata": {
        "id": "caa142fa-b375-4e78-b392-2072ced666f3"
      },
      "outputs": [],
      "source": [
        "# Llama 3.2 1B\n",
        "\n",
        "LLAMA32_CONFIG = {\n",
        "    \"vocab_size\": 128_256,           # Vocabulary size\n",
        "    \"context_length\": 131_072,       # Context length that was used to train the model\n",
        "    \"emb_dim\": 2048,                 # Embedding dimension\n",
        "    \"n_heads\": 32,                   # Number of attention heads\n",
        "    \"n_layers\": 16,                  # Number of layers\n",
        "    \"hidden_dim\": 8192,              # Size of the intermediate dimension in FeedForward\n",
        "    \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
        "    \"rope_base\": 500_000.0,          # The base in RoPE's \"theta\"\n",
        "    \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
        "    \"rope_freq\": {                   # RoPE frequency scaling\n",
        "        \"factor\": 32.0,\n",
        "        \"low_freq_factor\": 1.0,\n",
        "        \"high_freq_factor\": 4.0,\n",
        "        \"original_context_length\": 8192,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Llama 3.2 3B\n",
        "\n",
        "# LLAMA32_CONFIG = {\n",
        "#     \"vocab_size\": 128_256,           # Vocabulary size\n",
        "#     \"context_length\": 131_072,       # Context length that was used to train the model\n",
        "#     \"emb_dim\": 3072,                 # Embedding dimension\n",
        "#     \"n_heads\": 24,                   # Number of attention heads\n",
        "#     \"n_layers\": 28,                  # Number of layers\n",
        "#     \"hidden_dim\": 8192,              # Size of the intermediate dimension in FeedForward\n",
        "#     \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
        "#     \"rope_base\": 500_000.0,          # The base in RoPE's \"theta\"\n",
        "#     \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
        "#     \"rope_freq\": {                   # RoPE frequency scaling\n",
        "#         \"factor\": 32.0,\n",
        "#         \"low_freq_factor\": 1.0,\n",
        "#         \"high_freq_factor\": 4.0,\n",
        "#         \"original_context_length\": 8192,\n",
        "#     }\n",
        "# }\n",
        "\n",
        "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"emb_dim\"] == 2048 else \"3B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
      "metadata": {
        "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
      },
      "outputs": [],
      "source": [
        "model = Llama3Model(LLAMA32_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19de6c2c-83ce-456d-8be9-6ec415fe9eb1",
      "metadata": {
        "id": "19de6c2c-83ce-456d-8be9-6ec415fe9eb1"
      },
      "source": [
        "- The following is expected to print True to confirm buffers are reused instead of being (wastefully) recreated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
        "outputId": "bcbb0335-f154-4de2-da7a-e1a504292e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 1,498,482,688\n",
            "\n",
            "Total number of unique parameters: 1,235,814,400\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Account for weight tying\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
        "outputId": "8f16f74a-98d1-47c6-fdee-d80f05d3db08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32 (PyTorch default): 11.23 GB\n",
            "bfloat16: 5.61 GB\n",
            "If quantization is enabled above, actual memory usage will be lower.\n"
          ]
        }
      ],
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        # Calculate total number of elements per parameter\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        # Check if gradients are stored for this parameter\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    # Calculate buffer size (non-parameters that require memory)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "    # We assume parameters and gradients are stored in the same type as input dtype\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "    # Convert bytes to gigabytes\n",
        "    total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    return total_memory_gb\n",
        "\n",
        "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")\n",
        "print(\"If quantization is enabled above, actual memory usage will be lower.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
      "metadata": {
        "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e091e1-afa8-4d23-9aea-cced86181bfd",
      "metadata": {
        "id": "78e091e1-afa8-4d23-9aea-cced86181bfd"
      },
      "source": [
        "&nbsp;\n",
        "# 3. Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9482b01c-49f9-48e4-ab2c-4a4c75240e77",
      "metadata": {
        "id": "9482b01c-49f9-48e4-ab2c-4a4c75240e77"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    \"\"\"Thin wrapper around tiktoken that keeps track of Llama-3 special IDs.\"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        if not os.path.isfile(model_path):\n",
        "            raise FileNotFoundError(model_path)\n",
        "\n",
        "        mergeable = load_tiktoken_bpe(model_path)\n",
        "\n",
        "        # hard-coded from Meta's tokenizer.json\n",
        "        self.special = {\n",
        "            \"<|begin_of_text|>\": 128000,\n",
        "            \"<|end_of_text|>\": 128001,\n",
        "            \"<|start_header_id|>\": 128006,\n",
        "            \"<|end_header_id|>\": 128007,\n",
        "            \"<|eot_id|>\": 128009,\n",
        "        }\n",
        "        self.special.update({f\"<|reserved_{i}|>\": 128002 + i\n",
        "                             for i in range(256)\n",
        "                             if 128002 + i not in self.special.values()})\n",
        "\n",
        "        self.model = tiktoken.Encoding(\n",
        "            name=Path(model_path).name,\n",
        "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)\"\n",
        "                    r\"|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+\"\n",
        "                    r\"|\\p{N}{1,3}\"\n",
        "                    r\"| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*\"\n",
        "                    r\"|\\s*[\\r\\n]+\"\n",
        "                    r\"|\\s+(?!\\S)\"\n",
        "                    r\"|\\s+\",\n",
        "            mergeable_ranks=mergeable,\n",
        "            special_tokens=self.special,\n",
        "        )\n",
        "\n",
        "    def encode(self, text, bos=False, eos=False):\n",
        "        ids = ([self.special[\"<|begin_of_text|>\"]] if bos else []) \\\n",
        "              + self.model.encode(text)\n",
        "        if eos:\n",
        "            ids.append(self.special[\"<|end_of_text|>\"])\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self.model.decode(ids)\n",
        "\n",
        "\n",
        "class ChatFormat:\n",
        "\n",
        "    def __init__(self, tokenizer: Tokenizer, *,\n",
        "                 default_system=\"You are a helpful assistant.\"):\n",
        "        self.tok = tokenizer\n",
        "        self.default_system = default_system\n",
        "\n",
        "    def _header(self, role):\n",
        "        \"\"\"Encode <|start_header_id|>role<|end_header_id|>\\n\\n\"\"\"\n",
        "        return (\n",
        "            [self.tok.special[\"<|start_header_id|>\"]]\n",
        "            + self.tok.encode(role)\n",
        "            + [self.tok.special[\"<|end_header_id|>\"]]\n",
        "            + self.tok.encode(\"\\n\\n\")\n",
        "        )\n",
        "\n",
        "    def encode(self, user_message, system_message=None):\n",
        "        sys_msg = system_message if system_message is not None else self.default_system\n",
        "\n",
        "        ids = [self.tok.special[\"<|begin_of_text|>\"]]\n",
        "\n",
        "        # system\n",
        "        ids += self._header(\"system\")\n",
        "        ids += self.tok.encode(sys_msg)\n",
        "        ids += [self.tok.special[\"<|eot_id|>\"]]\n",
        "\n",
        "        # user\n",
        "        ids += self._header(\"user\")\n",
        "        ids += self.tok.encode(user_message)\n",
        "        ids += [self.tok.special[\"<|eot_id|>\"]]\n",
        "\n",
        "        # assistant header (no content yet)\n",
        "        ids += self._header(\"assistant\")\n",
        "\n",
        "        return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e9d96dc8-603a-4cb5-8c3e-4d2ca56862ed",
      "metadata": {
        "id": "e9d96dc8-603a-4cb5-8c3e-4d2ca56862ed"
      },
      "outputs": [],
      "source": [
        "#from huggingface_hub import login\n",
        "#login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "986bc1a0-804f-4154-80f8-44cefbee1368",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ec278d4edbe9423f9563a794e6f808bd",
            "7f3effc421cb41ff938a6fec1e1ba2aa",
            "afd5f9c83fcf48f389092271464f6316",
            "7cdb494931414f24ad33a462d28361a6",
            "e8280079401744c3bbbfd4ee38e10647",
            "936c69baa5724220826e01c65d14fef3",
            "dbea2862ecc14f8ea47dd3141803c46e",
            "bf9566ed08af475cb9899f267aa27fb8",
            "e1795a40167449ac84f42b48bee1946b",
            "09b8ece1fb3244978b7e4d4b304edc76",
            "67613eee06d44967bc053799ab7b219f"
          ]
        },
        "id": "986bc1a0-804f-4154-80f8-44cefbee1368",
        "outputId": "1627dee7-8b7b-45f7-d2df-300e13463808"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "original/tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec278d4edbe9423f9563a794e6f808bd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "tokenizer_file_path = hf_hub_download(\n",
        "    repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
        "    filename=\"original/tokenizer.model\",\n",
        "    local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "_gBhxDtU_nxo",
      "metadata": {
        "id": "_gBhxDtU_nxo"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(tokenizer_file_path)\n",
        "chat_tokenizer = ChatFormat(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c172f89f-d301-439f-b809-46169e5f5945",
      "metadata": {
        "id": "c172f89f-d301-439f-b809-46169e5f5945"
      },
      "source": [
        "&nbsp;\n",
        "# 4. Load pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "75166128-5899-4995-9b88-9672e135650e",
      "metadata": {
        "id": "75166128-5899-4995-9b88-9672e135650e"
      },
      "outputs": [],
      "source": [
        "def assign(left, right, tensor_name=\"unknown\"):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "    if isinstance(right, torch.Tensor):\n",
        "        return torch.nn.Parameter(right.clone().detach())\n",
        "    else:\n",
        "        return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_llama(model, param_config, params):\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
        "\n",
        "    for l in range(param_config[\"n_layers\"]):\n",
        "\n",
        "        # Load attention weights\n",
        "        model.trf_blocks[l].att.W_query.weight = assign(\n",
        "            model.trf_blocks[l].att.W_query.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_key.weight = assign(\n",
        "            model.trf_blocks[l].att.W_key.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_value.weight = assign(\n",
        "            model.trf_blocks[l].att.W_value.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
        "            model.trf_blocks[l].att.out_proj.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].norm1.weight = assign(\n",
        "            model.trf_blocks[l].norm1.weight,\n",
        "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.input_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "        # Load FeedForward weights\n",
        "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc1.weight,\n",
        "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc2.weight,\n",
        "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc3.weight,\n",
        "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
        "        )\n",
        "        model.trf_blocks[l].norm2.weight = assign(\n",
        "            model.trf_blocks[l].norm2.weight,\n",
        "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "    # Load output layer weights\n",
        "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
        "\n",
        "    if \"lm_head.weight\" in params.keys():\n",
        "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
        "    else:\n",
        "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
        "        print(\"Model uses weight tying.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c3b079a8e4ed4fe2857bf796afbc4525",
            "95038d82f35b4df2b4a5cf9659f545cd",
            "6bcaea7c415a40368a2f513ff686498a",
            "e36d51ef0be04fec95d18afa3002f496",
            "c0de131cf767495c8b5915248984d9c9",
            "d8ea3553ebeb4c65870915024297025f",
            "3f2b706e10384c4c972f63c02da3af70",
            "5a2c14fdcb48403d96fbe231a95c273c",
            "58b44aea025840fd836455bbfaa9837a",
            "afe97a6d1d054725ab20fef17fd4ba65",
            "ac0d1a443ceb466abc66f3966eed1940"
          ]
        },
        "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
        "outputId": "12be8c30-f408-48e0-8b96-4737f08b66ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3b079a8e4ed4fe2857bf796afbc4525"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model uses weight tying.\n"
          ]
        }
      ],
      "source": [
        "if LLAMA_SIZE_STR == \"1B\":\n",
        "    weights_file = hf_hub_download(\n",
        "        repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
        "        filename=\"model.safetensors\",\n",
        "        local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
        "    )\n",
        "    combined_weights = load_file(weights_file)\n",
        "\n",
        "\n",
        "else:\n",
        "    combined_weights = {}\n",
        "    for i in range(1, 3):\n",
        "        weights_file = hf_hub_download(\n",
        "            repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
        "            filename=f\"model-0000{i}-of-00002.safetensors\",\n",
        "            local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
        "        )\n",
        "        current_weights = load_file(weights_file)\n",
        "        combined_weights.update(current_weights)\n",
        "\n",
        "\n",
        "load_weights_into_llama(model, LLAMA32_CONFIG, combined_weights)\n",
        "model.to(device)\n",
        "del combined_weights  # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7f9f7ccc-70cb-41ff-9c25-44336042fc37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9f7ccc-70cb-41ff-9c25-44336042fc37",
        "outputId": "df767cba-0d8f-44f3-ab1a-cbd02b0c71b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight tying: True\n"
          ]
        }
      ],
      "source": [
        "print(\"Weight tying:\", torch.equal(model.tok_emb.weight, model.out_head.weight))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d07df1-4401-4792-b549-7c4cc5632323",
      "metadata": {
        "id": "57d07df1-4401-4792-b549-7c4cc5632323"
      },
      "source": [
        "&nbsp;\n",
        "# 5. Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
      "metadata": {
        "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
        "outputId": "e1b7dd8f-c7da-49f3-ad29-2ab3a77b6e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 21.00 sec\n",
            "Max memory allocated: 3.10 GB\n",
            "\n",
            "\n",
            "Output text:\n",
            "\n",
            " Quantization is a fundamental concept in physics and engineering that deals with the conversion of continuous signals into discrete values. In other words, it's the process of representing a continuous quantity, such as sound waves, light waves, or electrical signals, as a series of discrete values, known as quanta.\n",
            "\n",
            "In a continuous signal, such as a sound wave, a light wave, or an electrical signal, there are an infinite number of possible values that can be represented. However, in practice, these values are not always discrete, and it's often necessary to represent them as a series of discrete values, such as 0, 1, 2, 3, etc.\n",
            "\n",
            "Quantization is used in many areas, including:\n",
            "\n",
            "1. **Signal\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "PROMPT = \"What is quantization?\"\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(PROMPT, chat_tokenizer).to(device),\n",
        "    max_new_tokens=150,\n",
        "    context_size=LLAMA32_CONFIG[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(f\"Time: {time.time() - start:.2f} sec\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
        "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
        "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")\n",
        "\n",
        "output_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "\n",
        "def clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n",
        "    # Find the index of the first occurrence of \"<|end_header_id|>\"\n",
        "    index = text.find(header_end)\n",
        "\n",
        "    if index != -1:\n",
        "        # Return the substring starting after \"<|end_header_id|>\"\n",
        "        return text[index + len(header_end):].strip()  # Strip removes leading/trailing whitespace\n",
        "    else:\n",
        "        # If the token is not found, return the original text\n",
        "        return text\n",
        "\n",
        "print(\"\\n\\nOutput text:\\n\\n\", clean_text(output_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OIXkd8f5jxiR",
      "metadata": {
        "id": "OIXkd8f5jxiR"
      },
      "source": [
        "#LoRA fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "u_pRT6zsjwnS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_pRT6zsjwnS",
        "outputId": "4cf4e1eb-7ffe-4b47-93b1-d6919680eb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.8.9-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.8.8 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.8.8-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.55.2)\n",
            "Collecting datasets<4.0.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.10.0)\n",
            "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.34.4)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (0.21.4)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.8->unsloth) (0.10.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.8.8->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.8->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.8.8->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.8.9-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.8.8-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.8/184.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.28-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shtab, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 datasets-3.6.0 msgspec-0.19.0 shtab-1.7.2 trl-0.21.0 tyro-0.9.28 unsloth-2025.8.9 unsloth_zoo-2025.8.8 xformers-0.0.32.post2\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "-f_gFxDBk8vG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f_gFxDBk8vG",
        "outputId": "f51b65c0-2108-4fd6-aa77-f481f59f4f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U tensorboardX\n",
        "!pip install -q wandb\n",
        "!pip install -q -U torchvision\n",
        "!pip install -q -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d40bbf3",
      "metadata": {
        "id": "5d40bbf3"
      },
      "source": [
        "# Task\n",
        "Adapt the provided Python code to preprocess the \"Jofthomas/hermes-function-calling-thinking-V1\" dataset for fine-tuning a Llama 3.2 model using its specific tokenizer and chat template, and then prepare the data for training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91239c28",
      "metadata": {
        "id": "91239c28"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install `transformers` and `datasets` to load and process the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e15a4ddc",
      "metadata": {
        "id": "e15a4ddc"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries for loading and processing the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "23896491",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23896491",
        "outputId": "ffc8f054-83e5-4c56-a3ff-f56091488273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b79e3e03",
      "metadata": {
        "id": "b79e3e03"
      },
      "source": [
        "## Define a preprocessing function\n",
        "\n",
        "### Subtask:\n",
        "Adapt the `preprocess` function to work with your existing Llama 3.2 tokenizer and chat format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "056b0223",
      "metadata": {
        "id": "056b0223"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the preprocess function to encode the dataset examples using the chat tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d5643bcd",
      "metadata": {
        "id": "d5643bcd"
      },
      "outputs": [],
      "source": [
        "def preprocess(example):\n",
        "    # Use the chat_tokenizer to encode the conversation\n",
        "    # Assumes the dataset has a 'text' field containing the conversation in a format\n",
        "    # that chat_tokenizer can handle (e.g., a simple prompt/response structure).\n",
        "    # If the dataset has a different structure (e.g., 'prompt' and 'response' fields),\n",
        "    # you would need to adapt this to format the text appropriately before encoding.\n",
        "    # For this specific dataset 'Jofthomas/hermes-function-calling-thinking-V1',\n",
        "    # the 'text' field contains the entire conversation in a structured format.\n",
        "    encoded_text = chat_tokenizer.encode(example['text'])\n",
        "    return {'input_ids': encoded_text}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed34466",
      "metadata": {
        "id": "2ed34466"
      },
      "source": [
        "## Load and preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the dataset using `load_dataset` from the `datasets` library and apply the adapted preprocessing function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e81b29c",
      "metadata": {
        "id": "5e81b29c"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the dataset, apply the preprocessing function, and remove the original text column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9f8d2b3a",
      "metadata": {
        "id": "9f8d2b3a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f574e644",
      "metadata": {
        "id": "f574e644"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the dataset does not have a 'text' column. I need to inspect the dataset structure to understand what columns are available and how to construct the input text for the tokenizer from those columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "rUb8QKCtvo_h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUb8QKCtvo_h",
        "outputId": "84a898cf-4754-4cee-921a-a27c676733f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conversations']\n",
            "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\", 'role': 'system'}, {'content': 'Hi, can you tell me the current stock price of Apple?', 'role': 'human'}, {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\", 'role': 'model'}, {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\", 'role': 'tool'}, {'content': 'The current stock price of Apple is $150.75.', 'role': 'model'}, {'content': \"That's great. Can you also tell me the stock price for Microsoft?\", 'role': 'human'}, {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\", 'role': 'model'}, {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\", 'role': 'tool'}, {'content': 'The current stock price of Microsoft is $210.22.', 'role': 'model'}, {'content': 'Thank you for the information.', 'role': 'human'}, {'content': \"You're welcome! If you have any other questions, feel free to ask.\", 'role': 'model'}]}\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'].column_names)\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7b5757",
      "metadata": {
        "id": "1e7b5757"
      },
      "source": [
        "**Reasoning**:\n",
        "The dataset contains a 'conversations' column, which is a list of dictionaries, each representing a turn in the conversation with 'role' and 'content' keys. The previous `preprocess` function expected a 'text' column. I need to modify the `preprocess` function to iterate through the 'conversations' list, format it into a single string that the `chat_tokenizer` can understand, and then encode it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "rFJEzWGevx3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "2837bf570b504089bb4b75f1fe41745e",
            "891e4661ad7648a299a094f691579e8c",
            "d49a0993680143c78660d1c024c4eacb",
            "72d08541aefd4bcc874a472ceb2d9ad7",
            "5d301f64b9d34ad69d093055f471f918",
            "0e70f88f4246408e9f1e637fd807d0d5",
            "8dbaeadd48b040e6a45434b79b2f3db4",
            "c0e93667ada14422962125c6152d0889",
            "353b050b6ab9421ea3b36a51c3e85cf0",
            "27ef398f2e8e48be97b2c4b87d91e4ed",
            "e579d4bd598c4a4c903903dec1cd7e43"
          ]
        },
        "id": "rFJEzWGevx3a",
        "outputId": "33391709-7c81-4fa4-82e6-19d6c53e51eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2837bf570b504089bb4b75f1fe41745e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids'],\n",
            "        num_rows: 3570\n",
            "    })\n",
            "})\n",
            "{'input_ids': [128000, 128006, 9125, 128007, 271, 2675, 527, 264, 734, 8260, 15592, 1646, 13, 1472, 527, 3984, 449, 734, 33728, 2949, 366, 16297, 1500, 16297, 29, 12138, 9681, 39537, 1253, 1650, 832, 477, 810, 5865, 311, 7945, 449, 279, 1217, 3319, 13, 4418, 956, 1304, 32946, 922, 1148, 2819, 311, 20206, 1139, 5865, 91173, 527, 279, 2561, 7526, 32352, 16297, 29, 62208, 1337, 1232, 364, 1723, 518, 364, 1723, 1232, 5473, 609, 1232, 364, 456, 31641, 9217, 518, 364, 4789, 1232, 364, 1991, 279, 1510, 5708, 3430, 315, 264, 2883, 518, 364, 14105, 1232, 5473, 1337, 1232, 364, 1735, 518, 364, 13495, 1232, 5473, 10348, 1232, 5473, 1337, 1232, 364, 928, 518, 364, 4789, 1232, 364, 791, 836, 315, 279, 2883, 8439, 2186, 364, 6413, 1232, 2570, 10348, 663, 3500, 2186, 5473, 1337, 1232, 364, 1723, 518, 364, 1723, 1232, 5473, 609, 1232, 364, 456, 51829, 13563, 518, 364, 4789, 1232, 364, 1991, 3649, 922, 264, 5818, 518, 364, 14105, 1232, 5473, 1337, 1232, 364, 1735, 518, 364, 13495, 1232, 5473, 2150, 1232, 5473, 1337, 1232, 364, 928, 518, 364, 4789, 1232, 364, 791, 2316, 315, 279, 5818, 8439, 2186, 364, 6413, 1232, 2570, 2150, 663, 3500, 26516, 694, 16297, 29, 10464, 279, 2768, 4611, 67, 8322, 1646, 3024, 11036, 369, 1855, 5507, 1650, 499, 690, 1304, 25, 5473, 2150, 1232, 364, 5263, 7368, 518, 364, 1337, 1232, 364, 1735, 518, 364, 13495, 1232, 5473, 16774, 1232, 5473, 2150, 1232, 364, 19686, 518, 364, 1337, 1232, 364, 1735, 25762, 364, 609, 1232, 5473, 2150, 1232, 364, 678, 518, 364, 1337, 1232, 364, 928, 8439, 2186, 364, 6413, 1232, 2570, 16774, 518, 364, 609, 33841, 2520, 1855, 734, 1650, 471, 264, 3024, 1665, 449, 734, 836, 323, 6105, 2949, 366, 14506, 13735, 1500, 14506, 13735, 29, 12138, 9681, 439, 11263, 512, 27, 14506, 13735, 397, 90, 14506, 13735, 534, 524, 14506, 13735, 29, 128009, 128006, 26380, 128007, 271, 13347, 11, 649, 499, 3371, 757, 279, 1510, 5708, 3430, 315, 8325, 30, 128009, 128006, 78191, 128007, 271, 14023, 771, 29, 72586, 11, 279, 1217, 4691, 369, 279, 1510, 5708, 3430, 315, 8325, 13, 21815, 520, 279, 7526, 2561, 11, 1070, 596, 264, 734, 2663, 636, 31641, 9217, 902, 7041, 18809, 420, 1715, 13, 358, 3358, 1205, 311, 1650, 420, 734, 449, 279, 5852, 2883, 743, 311, 364, 27665, 6, 311, 17622, 279, 2038, 279, 1217, 374, 10371, 369, 13, 1115, 5084, 1093, 279, 1455, 31439, 323, 8475, 1957, 311, 1935, 627, 524, 27963, 1822, 14506, 13735, 397, 13922, 609, 1232, 364, 456, 31641, 9217, 518, 364, 16774, 1232, 5473, 10348, 1232, 364, 27665, 75591, 524, 14506, 13735, 29, 128009, 27, 14506, 9852, 397, 13922, 13787, 9217, 1232, 8412, 3965, 13, 2075, 16823, 524, 14506, 9852, 29, 128009, 128006, 78191, 128007, 271, 791, 1510, 5708, 3430, 315, 8325, 374, 400, 3965, 13, 2075, 13, 128009, 128006, 26380, 128007, 271, 4897, 596, 2294, 13, 3053, 499, 1101, 3371, 757, 279, 5708, 3430, 369, 5210, 30, 128009, 128006, 78191, 128007, 271, 14023, 771, 29, 72586, 11, 1095, 757, 1456, 311, 7216, 704, 1268, 311, 3790, 420, 1217, 596, 1715, 13, 578, 1217, 1176, 4691, 922, 8325, 596, 5708, 3430, 11, 323, 358, 16846, 555, 8260, 279, 636, 31641, 9217, 734, 449, 364, 27665, 6, 439, 279, 5811, 13, 578, 5507, 3984, 279, 3430, 11, 323, 358, 5946, 430, 2038, 1203, 311, 279, 1217, 382, 7184, 11, 279, 1217, 374, 10371, 369, 279, 5708, 3430, 315, 5210, 13, 358, 1205, 311, 8417, 279, 8475, 1957, 1618, 13, 21815, 520, 279, 7526, 2561, 11, 1070, 596, 2103, 279, 636, 31641, 9217, 734, 11, 902, 374, 6319, 311, 7963, 1510, 5708, 7729, 13, 8876, 279, 1217, 374, 35792, 2500, 2883, 596, 5708, 3430, 11, 433, 3727, 5647, 311, 1005, 279, 1890, 734, 1578, 719, 449, 279, 2883, 836, 5614, 311, 5210, 13, 4815, 40, 13434, 956, 9855, 904, 1023, 7526, 527, 4460, 1606, 279, 2561, 5865, 527, 11951, 41891, 369, 5708, 7729, 323, 5818, 3649, 13, 763, 420, 1162, 11, 279, 9959, 734, 374, 636, 31641, 9217, 13, 358, 1288, 6106, 430, 279, 734, 374, 2663, 449, 279, 4495, 5852, 11, 364, 13068, 6, 304, 420, 1162, 11, 311, 17622, 279, 13687, 2038, 279, 1217, 374, 11125, 13, 1115, 5603, 374, 11297, 323, 5398, 82, 6089, 449, 279, 1217, 596, 1715, 2085, 3339, 26225, 32946, 477, 1701, 46305, 7526, 627, 524, 27963, 1822, 14506, 13735, 397, 13922, 609, 1232, 364, 456, 31641, 9217, 518, 364, 16774, 1232, 5473, 10348, 1232, 364, 13068, 75591, 524, 14506, 13735, 29, 128009, 27, 14506, 9852, 397, 13922, 13787, 9217, 1232, 8412, 8848, 13, 1313, 16823, 524, 14506, 9852, 29, 128009, 128006, 78191, 128007, 271, 791, 1510, 5708, 3430, 315, 5210, 374, 400, 8848, 13, 1313, 13, 128009, 128006, 26380, 128007, 271, 13359, 499, 369, 279, 2038, 13, 128009, 128006, 78191, 128007, 271, 2675, 2351, 10788, 0, 1442, 499, 617, 904, 1023, 4860, 11, 2733, 1949, 311, 2610, 13, 128009]}\n"
          ]
        }
      ],
      "source": [
        "def preprocess(example):\n",
        "    conversation = example['conversations']\n",
        "\n",
        "    # Manually format the conversation turns into a single string, including special tokens\n",
        "    formatted_text = \"\"\n",
        "    for i, turn in enumerate(conversation):\n",
        "        role = turn['role']\n",
        "        content = turn['content']\n",
        "\n",
        "        # Add the beginning of text token at the very start of the first turn\n",
        "        if i == 0:\n",
        "             formatted_text += \"<|begin_of_text|>\"\n",
        "\n",
        "        # Add header and content based on the role\n",
        "        if role in ['system', 'human']:\n",
        "             formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'model':\n",
        "             # Map 'model' role to 'assistant' for the tokenizer\n",
        "             formatted_text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'tool':\n",
        "             # Include tool responses as content, followed by an EOT token\n",
        "             # We are assuming the tokenizer understands these tokens within the flow.\n",
        "             formatted_text += f\"{content}<|eot_id|>\"\n",
        "\n",
        "    # Encode the formatted text using the base tokenizer, allowing special tokens\n",
        "    # We need to explicitly allow the special tokens that we manually added to the formatted_text\n",
        "    allowed_special_tokens = set(tokenizer.special.keys())\n",
        "    encoded_text = tokenizer.model.encode(formatted_text, allowed_special=allowed_special_tokens)\n",
        "\n",
        "    return {'input_ids': encoded_text}\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")\n",
        "\n",
        "# Apply the preprocess function\n",
        "processed_dataset = dataset.map(preprocess, batched=False) # Set batched=False as the preprocess function processes one example at a time\n",
        "\n",
        "# Remove the original conversations column\n",
        "processed_dataset = processed_dataset.remove_columns('conversations')\n",
        "\n",
        "# Print some information about the processed dataset\n",
        "print(processed_dataset)\n",
        "print(processed_dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44ad816",
      "metadata": {
        "id": "d44ad816"
      },
      "source": [
        "## Prepare data for training\n",
        "\n",
        "### Subtask:\n",
        "Further process the preprocessed data into a format suitable for training your Llama 3.2 model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177bc887",
      "metadata": {
        "id": "177bc887"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine the maximum sequence length, pad or truncate the sequences, create attention masks, and format the data for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2Fee5kEuwLD9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "a0b95c60b9964df59d57ab4f4667ab89",
            "b8c8ff4dde2649028418bae5b047b1cb",
            "25f9f214d3564e389fd3350f7fffc98e",
            "3ddae24cfec2486a920ee454293d7bda",
            "235eabeb9a32456ebac20f07b0041b85",
            "18f483127acf4cffbcc39bbe9c303991",
            "4c5b8df48b2d45f7bc957e0e6ae91ee6",
            "3f519d23df404cbf9441b3fcfc61ec65",
            "d3e679e48b334aa8b8fee4c17a4e1bb8",
            "86e6ce73d50248f19e69514e3542454c",
            "eceec8d5f0ca49e596ac42452e569403",
            "979a1ede1af24a4389bf112a8780398c",
            "2c3efaf8124a4349ab576103f05d8b3c",
            "8cdfe02fae2c4f4d84f78833d0c21ce4",
            "921649d4a4d44d228cec8fa114a1ec43",
            "46acd092df4340bea59f44f3bd12b0f8",
            "f78124a2eac1483a8336d36354ead9e6",
            "ff64d646aa974baaa0e473798d918bd5",
            "5e24e70e9fbd4d0382120321bb52fe29",
            "0e2c1a81122e4f30b531a4d43735bcd1",
            "bcdb0b974a0c4e3f8f124e4081170a09",
            "1f5709530aff4de58bc29ab02b8d4c6a"
          ]
        },
        "id": "2Fee5kEuwLD9",
        "outputId": "d59e567f-7854-4f96-c12d-5fa450ad9ede"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0b95c60b9964df59d57ab4f4667ab89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "979a1ede1af24a4389bf112a8780398c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and formatted datasets:\n",
            "<__main__.LlamaDataset object at 0x7fd1ecdc2330>\n",
            "\n",
            "Sample from training dataset:\n",
            "{'input_ids': tensor([128000, 128006,   9125,  ..., 128009, 128009, 128009]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([128000, 128006,   9125,  ..., 128009, 128009, 128009])}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 1. Determine the maximum sequence length\n",
        "# Consider the model's context length and potentially available memory.\n",
        "# For simplicity, we will use a value that is a power of 2 and less than the model's context length.\n",
        "# A smaller context length can be used for faster training and less memory usage.\n",
        "max_sequence_length = 1024 # Example: Using a smaller sequence length for demonstration\n",
        "\n",
        "# 2. Pad or truncate the sequences\n",
        "def pad_and_truncate(example, max_length, pad_token_id):\n",
        "    input_ids = example['input_ids']\n",
        "    if len(input_ids) > max_length:\n",
        "        input_ids = input_ids[:max_length]\n",
        "    elif len(input_ids) < max_length:\n",
        "        padding = [pad_token_id] * (max_length - len(input_ids))\n",
        "        input_ids = input_ids + padding\n",
        "    return {'input_ids': input_ids}\n",
        "\n",
        "# Use the <|eot_id|> as the padding token\n",
        "pad_token_id = tokenizer.special[\"<|eot_id|>\"]\n",
        "\n",
        "# Apply padding and truncation\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: pad_and_truncate(example, max_sequence_length, pad_token_id),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# 3. Create attention masks\n",
        "def create_attention_mask(example, pad_token_id):\n",
        "    input_ids = example['input_ids']\n",
        "    # Mask is 1 for non-padding tokens and 0 for padding tokens\n",
        "    attention_mask = [1 if token_id != pad_token_id else 0 for token_id in input_ids]\n",
        "    return {'attention_mask': attention_mask}\n",
        "\n",
        "# Apply attention mask creation\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: create_attention_mask(example, pad_token_id),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# 4. Format the processed dataset into a PyTorch tensor dataset\n",
        "class LlamaDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.input_ids = [torch.tensor(x, dtype=torch.long) for x in dataset['input_ids']]\n",
        "        self.attention_mask = [torch.tensor(x, dtype=torch.long) for x in dataset['attention_mask']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.input_ids[idx].clone() # For language modeling, labels are the same as input_ids\n",
        "        }\n",
        "\n",
        "train_dataset = LlamaDataset(processed_dataset['train'])\n",
        "\n",
        "# Check if a 'test' split exists before creating eval_dataset\n",
        "eval_dataset = None\n",
        "if 'test' in processed_dataset:\n",
        "    eval_dataset = LlamaDataset(processed_dataset['test'])\n",
        "\n",
        "print(\"Processed and formatted datasets:\")\n",
        "print(train_dataset)\n",
        "if eval_dataset:\n",
        "    print(eval_dataset)\n",
        "print(\"\\nSample from training dataset:\")\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe918ba5",
      "metadata": {
        "id": "fe918ba5"
      },
      "source": [
        "## Define training loop\n",
        "\n",
        "### Subtask:\n",
        "Implement a training loop to fine-tune the Llama 3.2 model using the prepared dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "C6o4BFbbw1hi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497,
          "referenced_widgets": [
            "71eaf1a9f39344919d791ce9811c00d9",
            "563de4862c6946e99cd994997aefde9e",
            "eb3ce39f1ea54b89b8f61bccfe0dc8b7",
            "0699cbf0e30a4536951739f85d8255fb",
            "756fc636e7c249aca1164bca8fbceac8",
            "b34c9cfc11b9464e87dafc0d57f6a4bb",
            "ac06c01db4664d9fade6c6f9d8b27a40",
            "f8532cc226064ff8a61109785a3b3994",
            "f694de77655949b08b56da8654a35bb9",
            "f390880891264d6c98ab491038fbd49d",
            "6f4c4f1fc6364a72b4b6d9f2fac2b875",
            "ffde43ce87694219a48368d1e9d296fc",
            "f04cfe9e63d648bea3e450b3314e471a",
            "6afe3e27ff434a179d72dfa84cca8ac4",
            "dbef809f846e48aeb68628c2ad9d3a71",
            "2d9b716ffff24f029897f2a41cc51fdc",
            "6a5bdbcf18ab42e5b8bbbd96fe663c93",
            "690e81278cb648898bff3108254319d6",
            "1ed00af4e9414b8f802a059ea4f6eeee",
            "ad50c2a77dc54e21afbcf24c26aff82a",
            "8d92721d56594810a90674a9fb1651fb",
            "c05fcb552be34581aa1206cb716e1b62",
            "e220f7b234e045b8bc1213de148f5afe",
            "09d2541f239a4a768334937beb1bd146",
            "0cab5303db4b47518f560a0d70772b02",
            "1fda3029be2648939ac8e42d55906ff5",
            "866bb387f0a44ce18d340e8cb22afc7e",
            "cf907f5b8eb7449f95da3d2034bd9956",
            "0417f7fecec34e34aa6be9f517f22baa",
            "85256f1ab686440381e7bf0a377ff287",
            "e6d2c9a79b7a4a32b320e1c1dba725d0",
            "9ea851a2e27c4442b57fd6eabb8acd39",
            "c514d4fb28204fbf8f0ac4430a662c58"
          ]
        },
        "id": "C6o4BFbbw1hi",
        "outputId": "8d73e394-922a-4d58-8239-a6b520be69cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71eaf1a9f39344919d791ce9811c00d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffde43ce87694219a48368d1e9d296fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e220f7b234e045b8bc1213de148f5afe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 1 epochs with batch size 1 and sequence length 512...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 430.12 MiB is free. Process 11793 has 14.32 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 523.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1316966383.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# e. Update the model's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# f. Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_max_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 430.12 MiB is free. Process 11793 has 14.32 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 523.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "from datasets import load_dataset # Need to re-import load_dataset\n",
        "\n",
        "# Clear memory before retrying\n",
        "if 'train_dataloader' in locals():\n",
        "    del train_dataloader\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "# 1. Define the training parameters (Reduced sequence length)\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 1\n",
        "batch_size = 1 # Keeping batch size at 1 for now\n",
        "\n",
        "# Reduce the maximum sequence length\n",
        "max_sequence_length = 512 # Reduced sequence length\n",
        "\n",
        "# Load the dataset again\n",
        "dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")\n",
        "\n",
        "# Define the preprocess function again (from previous successful step)\n",
        "def preprocess(example):\n",
        "    conversation = example['conversations']\n",
        "    formatted_text = \"\"\n",
        "    for i, turn in enumerate(conversation):\n",
        "        role = turn['role']\n",
        "        content = turn['content']\n",
        "        if i == 0:\n",
        "             formatted_text += \"<|begin_of_text|>\"\n",
        "        if role in ['system', 'human']:\n",
        "             formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'model':\n",
        "             formatted_text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'tool':\n",
        "             formatted_text += f\"{content}<|eot_id|>\"\n",
        "\n",
        "    allowed_special_tokens = set(tokenizer.special.keys())\n",
        "    encoded_text = tokenizer.model.encode(formatted_text, allowed_special=allowed_special_tokens)\n",
        "    return {'input_ids': encoded_text}\n",
        "\n",
        "# Apply the preprocess function\n",
        "processed_dataset = dataset.map(preprocess, batched=False)\n",
        "processed_dataset = processed_dataset.remove_columns('conversations')\n",
        "\n",
        "\n",
        "# Define the padding and truncation function again (from previous successful step)\n",
        "def pad_and_truncate(example, max_length, pad_token_id):\n",
        "    input_ids = example['input_ids']\n",
        "    if len(input_ids) > max_length:\n",
        "        input_ids = input_ids[:max_length]\n",
        "    elif len(input_ids) < max_length:\n",
        "        padding = [pad_token_id] * (max_length - len(input_ids))\n",
        "        input_ids = input_ids + padding\n",
        "    return {'input_ids': input_ids}\n",
        "\n",
        "# Use the <|eot_id|> as the padding token\n",
        "pad_token_id = tokenizer.special[\"<|eot_id|>\"]\n",
        "\n",
        "# Apply padding and truncation to the processed_dataset\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: pad_and_truncate(example, max_sequence_length, pad_token_id),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Define the attention mask creation function again (from previous successful step)\n",
        "def create_attention_mask(example, pad_token_id):\n",
        "    input_ids = example['input_ids']\n",
        "    attention_mask = [1 if token_id != pad_token_id else 0 for token_id in input_ids]\n",
        "    return {'attention_mask': attention_mask}\n",
        "\n",
        "# Re-create attention masks with the new sequence length and apply to processed_dataset\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: create_attention_mask(example, pad_token_id),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Define the LlamaDataset class again (from previous successful step)\n",
        "class LlamaDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.input_ids = [torch.tensor(x, dtype=torch.long) for x in dataset['input_ids']]\n",
        "        self.attention_mask = [torch.tensor(x, dtype=torch.long) for x in dataset['attention_mask']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.input_ids[idx].clone()\n",
        "        }\n",
        "\n",
        "# Re-create the training dataset with the updated processed data\n",
        "train_dataset = LlamaDataset(processed_dataset['train'])\n",
        "\n",
        "\n",
        "# 2. Create a PyTorch DataLoader for the training dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs with batch size {batch_size} and sequence length {max_sequence_length}...\")\n",
        "\n",
        "# 4. Iterate through the specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    # 5. Within each epoch, iterate through the DataLoader to get batches of data\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        # 6. For each batch:\n",
        "        # a. Move the input IDs, attention masks, and labels to the appropriate device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # b. Perform a forward pass through the model\n",
        "        logits = model(input_ids)\n",
        "\n",
        "        # c. Calculate the loss\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        # d. Perform backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # e. Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # f. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 7. (Optional) Implement periodic logging of the training loss\n",
        "        if (i + 1) % 100 == 0: # Log every 100 batches\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 8. (Optional) Implement saving the model checkpoints periodically\n",
        "    # You can add code here to save the model state dictionary\n",
        "    # Example: torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a88299",
      "metadata": {
        "id": "24a88299"
      },
      "source": [
        "## Evaluate the fine-tuned model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the fine-tuned model on a test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73dfd47",
      "metadata": {
        "id": "a73dfd47"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the fine-tuned model on the evaluation dataset if it exists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b58b23dc",
      "metadata": {
        "id": "b58b23dc"
      },
      "outputs": [],
      "source": [
        "# 1. Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 2. If an evaluation dataset (eval_dataset) was created in the \"Prepare data for training\" step, create a PyTorch DataLoader for it.\n",
        "eval_dataloader = None\n",
        "if eval_dataset:\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle evaluation data\n",
        "\n",
        "# 3. Iterate through the evaluation DataLoader (if it exists).\n",
        "if eval_dataloader:\n",
        "    print(\"\\nStarting evaluation...\")\n",
        "    total_eval_loss = 0\n",
        "    with torch.no_grad(): # 4b. Perform a forward pass through the model without calculating gradients\n",
        "        for i, batch in enumerate(eval_dataloader):\n",
        "            # 4a. Move the input IDs, attention masks, and labels to the appropriate device.\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # 4b. (Continued)\n",
        "            logits = model(input_ids)\n",
        "\n",
        "            # 4c. Calculate the loss using the same loss function (loss_fn) used for training.\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    # 5. Calculate and print the average loss over the entire evaluation dataset.\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Evaluation finished, Average Loss: {avg_eval_loss:.4f}\")\n",
        "else:\n",
        "    # 6. If no evaluation dataset exists, print a message indicating that evaluation cannot be performed.\n",
        "    print(\"\\nNo evaluation dataset found, skipping evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VihfERoi-iHy",
      "metadata": {
        "id": "VihfERoi-iHy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from safetensors.torch import load_file\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tiktoken\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "from huggingface_hub import hf_hub_download\n",
        "from datasets import load_dataset # Need to re-import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "\n",
        "# Re-initialize the model with the defined configuration\n",
        "model = Llama3Model(LLAMA32_CONFIG)\n",
        "\n",
        "# Determine the device again\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Load the weights file again based on the model size\n",
        "if LLAMA_SIZE_STR == \"1B\":\n",
        "    weights_file = hf_hub_download(\n",
        "        repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
        "        filename=\"model.safetensors\",\n",
        "        local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
        "    )\n",
        "    combined_weights = load_file(weights_file)\n",
        "else:\n",
        "    combined_weights = {}\n",
        "    for i in range(1, 3):\n",
        "        weights_file = hf_hub_download(\n",
        "            repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
        "            filename=f\"model-0000{i}-of-00002.safetensors\",\n",
        "            local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
        "        )\n",
        "        current_weights = load_file(weights_file)\n",
        "        combined_weights.update(current_weights)\n",
        "\n",
        "# Load the weights into the re-initialized model\n",
        "load_weights_into_llama(model, LLAMA32_CONFIG, combined_weights)\n",
        "model.to(device)\n",
        "del combined_weights # free up memory\n",
        "\n",
        "# Re-load and preprocess the dataset to recreate eval_dataset\n",
        "# Define the preprocess function again (from previous successful step)\n",
        "def preprocess(example):\n",
        "    conversation = example['conversations']\n",
        "    formatted_text = \"\"\n",
        "    for i, turn in enumerate(conversation):\n",
        "        role = turn['role']\n",
        "        content = turn['content']\n",
        "        if i == 0:\n",
        "             formatted_text += \"<|begin_of_text|>\"\n",
        "        if role in ['system', 'human']:\n",
        "             formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'model':\n",
        "             formatted_text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'tool':\n",
        "             formatted_text += f\"{content}<|eot_id|>\"\n",
        "\n",
        "    allowed_special_tokens = set(tokenizer.special.keys())\n",
        "    encoded_text = tokenizer.model.encode(formatted_text, allowed_special=allowed_special_tokens)\n",
        "    return {'input_ids': encoded_text}\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")\n",
        "\n",
        "# Apply the preprocess function\n",
        "processed_dataset = dataset.map(preprocess, batched=False)\n",
        "processed_dataset = processed_dataset.remove_columns('conversations')\n",
        "\n",
        "# Define the padding and truncation function again (from previous successful step)\n",
        "def pad_and_truncate(example, max_length, pad_token_id):\n",
        "    input_ids = example['input_ids']\n",
        "    if len(input_ids) > max_length:\n",
        "        input_ids = input_ids[:max_length]\n",
        "    elif len(input_ids) < max_length:\n",
        "        padding = [pad_token_id] * (max_length - len(input_ids))\n",
        "        input_ids = input_ids + padding\n",
        "    return {'input_ids': input_ids}\n",
        "\n",
        "# Use the <|eot_id|> as the padding token\n",
        "pad_token_id = tokenizer.special[\"<|eot_id|>\"]\n",
        "\n",
        "# Apply padding and truncation to the processed_dataset\n",
        "max_sequence_length = 512 # Re-set the sequence length\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: pad_and_truncate(example, max_sequence_length, pad_token_id),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Define the attention mask creation function again (from previous successful step)\n",
        "def create_attention_mask(example, pad_token_id):\n",
        "    input_ids = example['input_ids']\n",
        "    attention_mask = [1 if token_id != pad_token_id else 0 for token_id in input_ids]\n",
        "    return {'attention_mask': attention_mask}\n",
        "\n",
        "# Re-create attention masks with the new sequence length and apply to processed_dataset\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: create_attention_mask(example, pad_token_id),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Define the LlamaDataset class again (from previous successful step)\n",
        "class LlamaDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.input_ids = [torch.tensor(x, dtype=torch.long) for x in dataset['input_ids']]\n",
        "        self.attention_mask = [torch.tensor(x, dtype=torch.long) for x in dataset['attention_mask']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.input_ids[idx].clone()\n",
        "        }\n",
        "\n",
        "# Re-create the training dataset with the updated processed data\n",
        "train_dataset = LlamaDataset(processed_dataset['train'])\n",
        "\n",
        "# Check if a 'test' split exists before creating eval_dataset\n",
        "eval_dataset = None\n",
        "if 'test' in processed_dataset:\n",
        "    eval_dataset = LlamaDataset(processed_dataset['test'])\n",
        "\n",
        "# Re-define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Now, proceed with evaluation as planned\n",
        "# 1. Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 2. If an evaluation dataset (eval_dataset) was created in the \"Prepare data for training\" step, create a PyTorch DataLoader for it.\n",
        "eval_dataloader = None\n",
        "batch_size = 1 # Re-set batch size for evaluation\n",
        "if eval_dataset:\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle evaluation data\n",
        "\n",
        "# 3. Iterate through the evaluation DataLoader (if it exists).\n",
        "if eval_dataloader:\n",
        "    print(\"\\nStarting evaluation...\")\n",
        "    total_eval_loss = 0\n",
        "    with torch.no_grad(): # 4b. Perform a forward pass through the model without calculating gradients\n",
        "        for i, batch in enumerate(eval_dataloader):\n",
        "            # 4a. Move the input IDs, attention masks, and labels to the appropriate device.\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # 4b. (Continued)\n",
        "            logits = model(input_ids)\n",
        "\n",
        "            # 4c. Calculate the loss using the same loss function (loss_fn) used for training.\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    # 5. Calculate and print the average loss over the entire evaluation dataset.\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Evaluation finished, Average Loss: {avg_eval_loss:.4f}\")\n",
        "else:\n",
        "    # 6. If no evaluation dataset exists, print a message indicating that evaluation cannot be performed.\n",
        "    print(\"\\nNo evaluation dataset found, skipping evaluation.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "My Environment Display Name",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec278d4edbe9423f9563a794e6f808bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f3effc421cb41ff938a6fec1e1ba2aa",
              "IPY_MODEL_afd5f9c83fcf48f389092271464f6316",
              "IPY_MODEL_7cdb494931414f24ad33a462d28361a6"
            ],
            "layout": "IPY_MODEL_e8280079401744c3bbbfd4ee38e10647"
          }
        },
        "7f3effc421cb41ff938a6fec1e1ba2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_936c69baa5724220826e01c65d14fef3",
            "placeholder": "​",
            "style": "IPY_MODEL_dbea2862ecc14f8ea47dd3141803c46e",
            "value": "original/tokenizer.model: 100%"
          }
        },
        "afd5f9c83fcf48f389092271464f6316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9566ed08af475cb9899f267aa27fb8",
            "max": 2183982,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1795a40167449ac84f42b48bee1946b",
            "value": 2183982
          }
        },
        "7cdb494931414f24ad33a462d28361a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b8ece1fb3244978b7e4d4b304edc76",
            "placeholder": "​",
            "style": "IPY_MODEL_67613eee06d44967bc053799ab7b219f",
            "value": " 2.18M/2.18M [00:00&lt;00:00, 2.96MB/s]"
          }
        },
        "e8280079401744c3bbbfd4ee38e10647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936c69baa5724220826e01c65d14fef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbea2862ecc14f8ea47dd3141803c46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf9566ed08af475cb9899f267aa27fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1795a40167449ac84f42b48bee1946b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09b8ece1fb3244978b7e4d4b304edc76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67613eee06d44967bc053799ab7b219f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3b079a8e4ed4fe2857bf796afbc4525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95038d82f35b4df2b4a5cf9659f545cd",
              "IPY_MODEL_6bcaea7c415a40368a2f513ff686498a",
              "IPY_MODEL_e36d51ef0be04fec95d18afa3002f496"
            ],
            "layout": "IPY_MODEL_c0de131cf767495c8b5915248984d9c9"
          }
        },
        "95038d82f35b4df2b4a5cf9659f545cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ea3553ebeb4c65870915024297025f",
            "placeholder": "​",
            "style": "IPY_MODEL_3f2b706e10384c4c972f63c02da3af70",
            "value": "model.safetensors: 100%"
          }
        },
        "6bcaea7c415a40368a2f513ff686498a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a2c14fdcb48403d96fbe231a95c273c",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58b44aea025840fd836455bbfaa9837a",
            "value": 2471645608
          }
        },
        "e36d51ef0be04fec95d18afa3002f496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afe97a6d1d054725ab20fef17fd4ba65",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0d1a443ceb466abc66f3966eed1940",
            "value": " 2.47G/2.47G [00:37&lt;00:00, 92.3MB/s]"
          }
        },
        "c0de131cf767495c8b5915248984d9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ea3553ebeb4c65870915024297025f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f2b706e10384c4c972f63c02da3af70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a2c14fdcb48403d96fbe231a95c273c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b44aea025840fd836455bbfaa9837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afe97a6d1d054725ab20fef17fd4ba65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0d1a443ceb466abc66f3966eed1940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2837bf570b504089bb4b75f1fe41745e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_891e4661ad7648a299a094f691579e8c",
              "IPY_MODEL_d49a0993680143c78660d1c024c4eacb",
              "IPY_MODEL_72d08541aefd4bcc874a472ceb2d9ad7"
            ],
            "layout": "IPY_MODEL_5d301f64b9d34ad69d093055f471f918"
          }
        },
        "891e4661ad7648a299a094f691579e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e70f88f4246408e9f1e637fd807d0d5",
            "placeholder": "​",
            "style": "IPY_MODEL_8dbaeadd48b040e6a45434b79b2f3db4",
            "value": "Map: 100%"
          }
        },
        "d49a0993680143c78660d1c024c4eacb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e93667ada14422962125c6152d0889",
            "max": 3570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_353b050b6ab9421ea3b36a51c3e85cf0",
            "value": 3570
          }
        },
        "72d08541aefd4bcc874a472ceb2d9ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27ef398f2e8e48be97b2c4b87d91e4ed",
            "placeholder": "​",
            "style": "IPY_MODEL_e579d4bd598c4a4c903903dec1cd7e43",
            "value": " 3570/3570 [00:04&lt;00:00, 881.24 examples/s]"
          }
        },
        "5d301f64b9d34ad69d093055f471f918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e70f88f4246408e9f1e637fd807d0d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dbaeadd48b040e6a45434b79b2f3db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0e93667ada14422962125c6152d0889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353b050b6ab9421ea3b36a51c3e85cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27ef398f2e8e48be97b2c4b87d91e4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e579d4bd598c4a4c903903dec1cd7e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0b95c60b9964df59d57ab4f4667ab89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8c8ff4dde2649028418bae5b047b1cb",
              "IPY_MODEL_25f9f214d3564e389fd3350f7fffc98e",
              "IPY_MODEL_3ddae24cfec2486a920ee454293d7bda"
            ],
            "layout": "IPY_MODEL_235eabeb9a32456ebac20f07b0041b85"
          }
        },
        "b8c8ff4dde2649028418bae5b047b1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f483127acf4cffbcc39bbe9c303991",
            "placeholder": "​",
            "style": "IPY_MODEL_4c5b8df48b2d45f7bc957e0e6ae91ee6",
            "value": "Map: 100%"
          }
        },
        "25f9f214d3564e389fd3350f7fffc98e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f519d23df404cbf9441b3fcfc61ec65",
            "max": 3570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3e679e48b334aa8b8fee4c17a4e1bb8",
            "value": 3570
          }
        },
        "3ddae24cfec2486a920ee454293d7bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86e6ce73d50248f19e69514e3542454c",
            "placeholder": "​",
            "style": "IPY_MODEL_eceec8d5f0ca49e596ac42452e569403",
            "value": " 3570/3570 [00:02&lt;00:00, 1681.63 examples/s]"
          }
        },
        "235eabeb9a32456ebac20f07b0041b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f483127acf4cffbcc39bbe9c303991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5b8df48b2d45f7bc957e0e6ae91ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f519d23df404cbf9441b3fcfc61ec65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e679e48b334aa8b8fee4c17a4e1bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86e6ce73d50248f19e69514e3542454c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eceec8d5f0ca49e596ac42452e569403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "979a1ede1af24a4389bf112a8780398c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c3efaf8124a4349ab576103f05d8b3c",
              "IPY_MODEL_8cdfe02fae2c4f4d84f78833d0c21ce4",
              "IPY_MODEL_921649d4a4d44d228cec8fa114a1ec43"
            ],
            "layout": "IPY_MODEL_46acd092df4340bea59f44f3bd12b0f8"
          }
        },
        "2c3efaf8124a4349ab576103f05d8b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78124a2eac1483a8336d36354ead9e6",
            "placeholder": "​",
            "style": "IPY_MODEL_ff64d646aa974baaa0e473798d918bd5",
            "value": "Map: 100%"
          }
        },
        "8cdfe02fae2c4f4d84f78833d0c21ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e24e70e9fbd4d0382120321bb52fe29",
            "max": 3570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e2c1a81122e4f30b531a4d43735bcd1",
            "value": 3570
          }
        },
        "921649d4a4d44d228cec8fa114a1ec43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcdb0b974a0c4e3f8f124e4081170a09",
            "placeholder": "​",
            "style": "IPY_MODEL_1f5709530aff4de58bc29ab02b8d4c6a",
            "value": " 3570/3570 [00:02&lt;00:00, 1299.71 examples/s]"
          }
        },
        "46acd092df4340bea59f44f3bd12b0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78124a2eac1483a8336d36354ead9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff64d646aa974baaa0e473798d918bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e24e70e9fbd4d0382120321bb52fe29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2c1a81122e4f30b531a4d43735bcd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcdb0b974a0c4e3f8f124e4081170a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f5709530aff4de58bc29ab02b8d4c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71eaf1a9f39344919d791ce9811c00d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_563de4862c6946e99cd994997aefde9e",
              "IPY_MODEL_eb3ce39f1ea54b89b8f61bccfe0dc8b7",
              "IPY_MODEL_0699cbf0e30a4536951739f85d8255fb"
            ],
            "layout": "IPY_MODEL_756fc636e7c249aca1164bca8fbceac8"
          }
        },
        "563de4862c6946e99cd994997aefde9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b34c9cfc11b9464e87dafc0d57f6a4bb",
            "placeholder": "​",
            "style": "IPY_MODEL_ac06c01db4664d9fade6c6f9d8b27a40",
            "value": "Map: 100%"
          }
        },
        "eb3ce39f1ea54b89b8f61bccfe0dc8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8532cc226064ff8a61109785a3b3994",
            "max": 3570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f694de77655949b08b56da8654a35bb9",
            "value": 3570
          }
        },
        "0699cbf0e30a4536951739f85d8255fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f390880891264d6c98ab491038fbd49d",
            "placeholder": "​",
            "style": "IPY_MODEL_6f4c4f1fc6364a72b4b6d9f2fac2b875",
            "value": " 3570/3570 [00:04&lt;00:00, 886.39 examples/s]"
          }
        },
        "756fc636e7c249aca1164bca8fbceac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b34c9cfc11b9464e87dafc0d57f6a4bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac06c01db4664d9fade6c6f9d8b27a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8532cc226064ff8a61109785a3b3994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f694de77655949b08b56da8654a35bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f390880891264d6c98ab491038fbd49d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4c4f1fc6364a72b4b6d9f2fac2b875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffde43ce87694219a48368d1e9d296fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f04cfe9e63d648bea3e450b3314e471a",
              "IPY_MODEL_6afe3e27ff434a179d72dfa84cca8ac4",
              "IPY_MODEL_dbef809f846e48aeb68628c2ad9d3a71"
            ],
            "layout": "IPY_MODEL_2d9b716ffff24f029897f2a41cc51fdc"
          }
        },
        "f04cfe9e63d648bea3e450b3314e471a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5bdbcf18ab42e5b8bbbd96fe663c93",
            "placeholder": "​",
            "style": "IPY_MODEL_690e81278cb648898bff3108254319d6",
            "value": "Map: 100%"
          }
        },
        "6afe3e27ff434a179d72dfa84cca8ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ed00af4e9414b8f802a059ea4f6eeee",
            "max": 3570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad50c2a77dc54e21afbcf24c26aff82a",
            "value": 3570
          }
        },
        "dbef809f846e48aeb68628c2ad9d3a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d92721d56594810a90674a9fb1651fb",
            "placeholder": "​",
            "style": "IPY_MODEL_c05fcb552be34581aa1206cb716e1b62",
            "value": " 3570/3570 [00:01&lt;00:00, 2039.12 examples/s]"
          }
        },
        "2d9b716ffff24f029897f2a41cc51fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a5bdbcf18ab42e5b8bbbd96fe663c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690e81278cb648898bff3108254319d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ed00af4e9414b8f802a059ea4f6eeee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad50c2a77dc54e21afbcf24c26aff82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d92721d56594810a90674a9fb1651fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05fcb552be34581aa1206cb716e1b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e220f7b234e045b8bc1213de148f5afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09d2541f239a4a768334937beb1bd146",
              "IPY_MODEL_0cab5303db4b47518f560a0d70772b02",
              "IPY_MODEL_1fda3029be2648939ac8e42d55906ff5"
            ],
            "layout": "IPY_MODEL_866bb387f0a44ce18d340e8cb22afc7e"
          }
        },
        "09d2541f239a4a768334937beb1bd146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf907f5b8eb7449f95da3d2034bd9956",
            "placeholder": "​",
            "style": "IPY_MODEL_0417f7fecec34e34aa6be9f517f22baa",
            "value": "Map: 100%"
          }
        },
        "0cab5303db4b47518f560a0d70772b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85256f1ab686440381e7bf0a377ff287",
            "max": 3570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6d2c9a79b7a4a32b320e1c1dba725d0",
            "value": 3570
          }
        },
        "1fda3029be2648939ac8e42d55906ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea851a2e27c4442b57fd6eabb8acd39",
            "placeholder": "​",
            "style": "IPY_MODEL_c514d4fb28204fbf8f0ac4430a662c58",
            "value": " 3570/3570 [00:01&lt;00:00, 2295.36 examples/s]"
          }
        },
        "866bb387f0a44ce18d340e8cb22afc7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf907f5b8eb7449f95da3d2034bd9956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0417f7fecec34e34aa6be9f517f22baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85256f1ab686440381e7bf0a377ff287": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d2c9a79b7a4a32b320e1c1dba725d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ea851a2e27c4442b57fd6eabb8acd39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c514d4fb28204fbf8f0ac4430a662c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}